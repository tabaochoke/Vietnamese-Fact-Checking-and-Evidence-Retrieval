{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6920588,"sourceType":"datasetVersion","datasetId":3973860},{"sourceId":6933873,"sourceType":"datasetVersion","datasetId":3981445}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install --quiet evaluate\n!pip install -U sentence-transformers\n!pip install --quiet underthesea\n!pip install accelerate","metadata":{"execution":{"iopub.status.busy":"2023-11-16T10:59:22.474888Z","iopub.execute_input":"2023-11-16T10:59:22.475177Z","iopub.status.idle":"2023-11-16T11:00:15.859061Z","shell.execute_reply.started":"2023-11-16T10:59:22.475143Z","shell.execute_reply":"2023-11-16T11:00:15.857825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer ,AutoModelForSequenceClassification, TrainingArguments, Trainer , AutoModelForQuestionAnswering , AdamW , get_scheduler , DataCollatorWithPadding\nfrom datasets import load_dataset , concatenate_datasets\n#from accelerate import Accelerator\nimport evaluate\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\n#from transformers import DataCollatorWithPadding\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit , train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom datasets import Dataset,DatasetDict\nimport collections\nimport evaluate\nfrom huggingface_hub import notebook_login\n# from underthesea import sent_tokenize ,  text_normalize , word_tokenize\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, util\nimport re\nimport torch.nn as nn\nimport pandas as pd\nimport seaborn as sns\nfrom underthesea import word_tokenize\nSEED = 1337","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T11:00:15.860926Z","iopub.execute_input":"2023-11-16T11:00:15.861242Z","iopub.status.idle":"2023-11-16T11:00:33.316633Z","shell.execute_reply.started":"2023-11-16T11:00:15.861214Z","shell.execute_reply":"2023-11-16T11:00:33.315707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pandas = pd.read_json (\"/kaggle/input/fold-1345/TRAIN CONTEXT SPLIT 3\")\nval_pandas = pd.read_json (\"/kaggle/input/fold-1345/Validation CONTEXT SPLIT 3\")\ntrain_pandas = train_pandas.drop (\"__index_level_0__\" , axis = \"columns\")\nval_pandas = val_pandas.drop (\"__index_level_0__\" , axis = \"columns\")\nds_dict = {'train' : Dataset.from_pandas(train_pandas),\n           'test' : Dataset.from_pandas(val_pandas)  }\nds = DatasetDict(ds_dict)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:00:33.317986Z","iopub.execute_input":"2023-11-16T11:00:33.318750Z","iopub.status.idle":"2023-11-16T11:00:39.678697Z","shell.execute_reply.started":"2023-11-16T11:00:33.318712Z","shell.execute_reply":"2023-11-16T11:00:39.677847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\ncheckpoint = \"VietAI/vit5-base\"\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint , num_labels = 3)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n# accuracy = evaluate.load(\"accuracy\")\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     predictions = np.argmax(predictions, axis=1)\n#     return accuracy.compute(predictions=predictions, references=labels)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:00:39.681258Z","iopub.execute_input":"2023-11-16T11:00:39.681692Z","iopub.status.idle":"2023-11-16T11:00:47.648041Z","shell.execute_reply.started":"2023-11-16T11:00:39.681626Z","shell.execute_reply":"2023-11-16T11:00:47.647114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_tok = \"<s>\"\nsep_tok = \"</s>\"\ndef fix_input (sample) :\n    return {\"input\" : cls_tok + ' ' + sample['claim'] + ' ' + sep_tok +  ' ' +sample['new_context'] + ' ' + sep_tok  }\ndata_fix_input = ds.map (fix_input )","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:00:47.649368Z","iopub.execute_input":"2023-11-16T11:00:47.650051Z","iopub.status.idle":"2023-11-16T11:00:55.132244Z","shell.execute_reply.started":"2023-11-16T11:00:47.650014Z","shell.execute_reply":"2023-11-16T11:00:55.131447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_fix_input['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:00:55.133324Z","iopub.execute_input":"2023-11-16T11:00:55.133625Z","iopub.status.idle":"2023-11-16T11:00:55.141154Z","shell.execute_reply.started":"2023-11-16T11:00:55.133599Z","shell.execute_reply":"2023-11-16T11:00:55.140263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize (sample) :\n    return tokenizer(sample [\"input\"] , truncation= True , max_length = 768)\ndata_tokenize = data_fix_input.map (tokenize , batched = True )\nfinal_data = data_tokenize.remove_columns (['context', 'claim', 'verdict', 'evidence', 'domain', '__index_level_0__', 'new_context','input'])\nfinal_data","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:00:55.142352Z","iopub.execute_input":"2023-11-16T11:00:55.142651Z","iopub.status.idle":"2023-11-16T11:01:09.987180Z","shell.execute_reply.started":"2023-11-16T11:00:55.142625Z","shell.execute_reply":"2023-11-16T11:01:09.986153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 3\n# device = torch.device (\"cuda\") if torch.cuda.is_available() else torch.device (\"cpu\" )\n# model.to(device)\n#Dataloader\ntrain_dataloader = DataLoader (final_data[\"train\"], batch_size=2, shuffle=True ,collate_fn = data_collator )\nval_dataloader = DataLoader (final_data[\"test\"], batch_size=8, shuffle=True , collate_fn = data_collator)\n# test_dataloader = DataLoader (split_dataset[\"test\"] , batch_size=16, shuffle=True , collate_fn = data_collator )\n#Scheduler\nstep_to_train = epochs * len (train_dataloader)\noptimizer = torch.optim.AdamW (model.parameters () , lr = 2e-5)\nscheduler = get_scheduler (name = \"cosine\" , \n                           optimizer = optimizer ,\n                           num_warmup_steps=0,\n                           num_training_steps = step_to_train )\n\n#accelerator\naccelerator = Accelerator ()\ntrain_dataloader ,val_dataloader ,model , optimizer= accelerator.prepare (train_dataloader  , val_dataloader , model , optimizer)\n\n#optimzier and scheduler\n# step_to_train = epochs * len (train_dataloader)\n# optimizer = torch.optim.AdamW (model.parameters () , lr = 5e-5)\n# scheduler = get_scheduler (name = \"linear\" , \n#                            optimizer = optimizer ,\n#                            num_warmup_steps=0,\n#                            num_training_steps = step_to_train\n#                           )\n\n#Metric\nmetric_train_acc = evaluate.load(\"accuracy\")\nmetric_valid_acc = evaluate.load(\"accuracy\")\nmetric_train_f1 = evaluate.load(\"f1\")\nmetric_valid_f1 = evaluate.load(\"f1\")\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     predictions = np.argmax(predictions, axis=1)\n#     return metric.compute(predictions=predictions, references=labels)\n#progress_bar = tqdm(range(step_to_train))\n\n     ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:01:09.988643Z","iopub.execute_input":"2023-11-16T11:01:09.989247Z","iopub.status.idle":"2023-11-16T11:01:17.661312Z","shell.execute_reply.started":"2023-11-16T11:01:09.989210Z","shell.execute_reply":"2023-11-16T11:01:17.660074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nbest_val_acc = 0\ni= 0 \nmodel.train ()\nfor epoch in range (epochs) :\n    losses = []\n    for batch in tqdm (train_dataloader)  :\n        labels = batch.pop (\"labels\")\n        output = model (**batch)\n        logits = output.logits\n        criterion = nn.CrossEntropyLoss()\n        #print (outputs.shape , labels.shape)\n        loss = criterion(logits,labels)\n        \n        accelerator.backward (loss)\n        \n        optimizer.step ()\n        scheduler.step ()\n        optimizer.zero_grad ()\n        \n        losses.append (loss.item())\n        predict = torch.argmax (logits, dim = -1)\n        metric_train_acc.add_batch (references=labels, predictions = predict)\n        metric_train_f1.add_batch (references=labels, predictions = predict)\n\n    print (\"Epoch : {} , Loss : {} , Accuracy : {} , F1 : {}\".format (epoch + 1 , np.mean( losses) , metric_train_acc.compute ()['accuracy'] , metric_train_f1.compute(average=\"weighted\")['f1'] ) )\n\n\n    model.eval()\n    val_losses = []\n\n    for batch in tqdm (val_dataloader) :\n        \n        with torch.no_grad () :\n            labels = batch.pop (\"labels\")\n            output = model (**batch)\n            logits = output.logits\n            criterion = nn.CrossEntropyLoss()\n            #print (outputs.shape , labels.shape)\n            loss = criterion(logits,labels)\n            val_losses.append (loss.item())\n            \n        predict = torch.argmax (output.logits , dim = -1)\n        metric_valid_acc.add_batch (references=labels, predictions = predict)\n        metric_valid_f1.add_batch (references=labels, predictions = predict )\n\n\n    val_acc = metric_valid_acc.compute ()['accuracy']\n    if val_acc > best_val_acc :\n        torch.save(model.state_dict(), \"Phobert_v2_best_model_full_data_{}\".format (i))\n        best_val_acc = val_acc\n        i+=1\n    print (\"Epoch : {}, Val Loss: {} , Validation  ACC Result : {} , Validation F1 Result :{}\".format (epoch + 1, np.mean( val_losses) , val_acc , metric_valid_f1.compute (average=\"weighted\")['f1'] ))\n                      ","metadata":{"execution":{"iopub.status.busy":"2023-11-16T11:01:17.663020Z","iopub.execute_input":"2023-11-16T11:01:17.663308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r\"Phobert_v2_best_model_full_data_1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}